{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import bz2\n",
    "import json\n",
    "import os.path\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "import tarfile\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import snapshot_download\n",
    "from windows_toasts import Toast, InteractableWindowsToaster\n",
    "\n",
    "from config import HOTPOT_QA_ROOT, HOTPOT_DOCUMENT_ROOT"
   ],
   "id": "bd98bf88b8ab061a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Download QA part of HotpotQA\n",
    "\n",
    "It contains 2 folders\n",
    "- fullwiki: QA without gold documents, used to test retrieval and generation\n",
    "- distractor: QA with gold documents, used to test generation\n",
    "\n",
    "However, two folders have same training and validation data, so we only use **fullwiki** part."
   ],
   "id": "5fb567f6f50bb333"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.exists(f'{HOTPOT_QA_ROOT}/fullwiki'):\n",
    "    print('Hotpot QA is already downloaded')\n",
    "else:\n",
    "    snapshot_download(\n",
    "        repo_id='hotpotqa/hotpot_qa',\n",
    "        local_dir=HOTPOT_QA_ROOT,\n",
    "        cache_dir='caches',\n",
    "        repo_type='dataset',\n",
    "    )"
   ],
   "id": "a94df6e16dd4844d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Examine QA samples\n",
    "\n",
    "Each QA has following fields:\n",
    "- id\n",
    "- question\n",
    "- answer\n",
    "- type: str; question type, either 'bridge' or 'comparison'\n",
    "    - 'bridge': Ask a fact, where the fact needs an intermediate entity(bridge) to retrieve\n",
    "    - 'comparison': Compare the same attribute of two entities\n",
    "- level: str; difficulty level, one of 'easy', 'medium', or 'hard'\n",
    "- supporting_facts: list\\[dict{'title', 'index'}]; The gold documents used to answer the question\n",
    "    - It gives titles and document indices, the actual sentences can be found in field **context** or HotPot documents(see next part).\n",
    "- context: list\\[dict{'title', 'sentences'}]; 10 documents with 2 gold and 8 distractor\n",
    "    - It gives titles and sentences"
   ],
   "id": "2d25814f60988f1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_parquet(f'{HOTPOT_QA_ROOT}/fullwiki/train-00000-of-00002.parquet')\n",
    "df.head(3)"
   ],
   "id": "53fea81d26ce88a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Download documents of HotpotQA",
   "id": "9896953f01c413b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download\n",
    "\n",
    "url = ('https://nlp.stanford.edu/projects/hotpotqa'\n",
    "       '/enwiki-20171001-pages-meta-current-withlinks-processed.tar.bz2')\n",
    "bz2_path = (f'{HOTPOT_DOCUMENT_ROOT}'\n",
    "            f'/enwiki-20171001-pages-meta-current-withlinks-processed.tar.bz2')\n",
    "\n",
    "if os.path.exists(bz2_path):\n",
    "    print('Hotpot document is already downloaded')\n",
    "else:\n",
    "    # Request downloading url\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Save file\n",
    "    total_size, chunk_size = int(response.headers['Content-Length']), 8192\n",
    "    with tqdm(total=total_size) as progress_bar:\n",
    "        with open(bz2_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "                    progress_bar.update(len(chunk))"
   ],
   "id": "8964016dc4c71ead",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Unzip and flatten\n",
    "\n",
    "if os.path.exists(f'{HOTPOT_DOCUMENT_ROOT}/0.bz2'):\n",
    "    print('Hotpot document is already unzipped and flattened')\n",
    "else:\n",
    "    # Unzip\n",
    "    file_size = os.path.getsize(bz2_path)\n",
    "    with open(bz2_path, 'rb') as file:\n",
    "        with tqdm.wrapattr(file, 'read', total=file_size) as file_wrapper:\n",
    "            with tarfile.open(fileobj=file_wrapper, mode=\"r:bz2\") as tar:\n",
    "                tar.extractall(HOTPOT_DOCUMENT_ROOT)\n",
    "    # Flatten\n",
    "    for i, data_path in tqdm(\n",
    "            enumerate(glob(f'{HOTPOT_DOCUMENT_ROOT}/**/*.bz2', recursive=True))\n",
    "    ):\n",
    "        shutil.move(data_path, f'{HOTPOT_DOCUMENT_ROOT}/{i}.bz2')\n",
    "    shutil.rmtree(bz2_path.replace('.tar.bz2', ''))"
   ],
   "id": "7da38050a3e9d5c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fe38dad980247b79",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
